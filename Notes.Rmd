---
title: "Financial Risk Management with R"
author: "Adair Neto"
date: \today
output: pdf_document
---

# Retrieving FRED Data

Which risk factors are important? 

What is the distribution of risk factor returns? Is it normal? If not, does it have heavy tails? 

How predictable are risk factor returns? Serial correlation? Volatility clustering? 

Important admin: add the command `RNGkind(sample.kind=”Rounding”)` before any instance of the `set.seed()`.

```{r}
library(quantmod)
wilsh <- getSymbols("WILL5000IND", src="FRED", auto.assign=FALSE)
wilsh <- na.omit(wilsh)
wilsh <- wilsh["1979-12-31/2017-12-31"]
names(wilsh) <- "TR"

head(wilsh, 3)
tail(wilsh, 3)
```

To compute daily returns, a natural idea is to define a **discrete return**:
$$
\text{ret}_t = \frac{\text{wilsh}_t}{\text{wilsh}_{t-1}} - 1
$$

However, this is not symmetric. Thus, we use the **log return** (also known as **continuously compound return**):
$$
\text{logret}_t = \log(1 + \text{ret}_t)
$$

It can be shown that 
$$
\text{logret}_t = \log(\text{wilsh}_t) - \log(\text{wilsh}_{t-1})
$$
and 
$$
\text{ret}_t = \exp(\text{logret}_t) - 1
$$

```{r}
logret <- diff(log(wilsh$TR))[-1]
round(head(logret,3),6)

ret <- exp(logret)-1
round(head(ret,3),6)
```

For longer periods of time, e.g. n-days, we compute $\text{logret}_t + \cdots + \text{logret}_{t-n+1}$ for the log return and $\exp(\text{logret}_t + \cdots + \text{logret}_{t-n+1}) - 1$ for the discrete return.

In R, we use the function `apply.weekly` to compute weekly log return. 

```{r}
logret_w <- apply.weekly(logret, sum)
round(head(logret_w,3), 6)
ret_w <- exp(logret_w) - 1
```

Also available: `apply.monthly`, `apply.quarterly`, and `apply.yearly`.

# Risk Management Under Normal Distributions

## Estimating $\mu$ and $\sigma$

Assume that $\text{logret} \sim N(\mu, \sigma)$.

We estimate $\mu$ using the sample mean and $\sigma$ using the sample standard deviation.

```{r}
mu <- mean(logret)
sigma <- sd(logret)
```

## Value-at-Risk (VaR)

**Definition. (Value-at-risk)** The VaR is the amount that a portfolio might lose, with a given probability $(1-\alpha)$ (confidence level), over a given time period.

The VaR is the alpha quantile of the pdf.

```{r}
alpha <- 0.05
var <- qnorm(alpha, mu, sigma)
```

**Example.** Suppose that a hedge fund is investing $1000 million in US equities. The VaR of the daily change in its assets, with 95% confidence level, is given by

```{r}
HFvar <- 1000 * (exp(var) - 1)
```

## Expected Shortfall (ES)

**Definition. (Expected Shortfall)** ES is the expected return given that the return is worse than the associated VaR.

The average loss is:

```{r}
es <- mu - sigma * dnorm(qnorm(alpha, 0, 1), 0, 1)/alpha
```

In the previous example, 

```{r}
HFes <- 1000 * (exp(es) - 1)
round(HFes, 1)
```

## Using Simulation to Estimate VaR and ES

We'll simulate some data, and take the $\alpha$-quantile of the simulated data.

One way of doing that is by drawing 100,000 outcomes from $N(\mu, \sigma)$ distribution. Here, we assume normality.

```{r}
RNGkind(sample.kind="Rounding")
set.seed(123789)

rvec <- rnorm(100000, mu, sigma)
VaR <- quantile(rvec, alpha)
ES <- mean(rvec[rvec < VaR])

round(VaR, 6)
round(ES, 6)
```

Another way: draw 100,000 outcomes (with replacement) from the vector of daily log returns. Here, we do not assume normality.

```{r}
RNGkind(sample.kind="Rounding")
set.seed(123789)

rvec <- sample(as.vector(logret), 100000, replace = TRUE)
VaR <- quantile(rvec, alpha)
ES <- mean(rvec[rvec < VaR])

round(VaR, 6)
round(ES, 6)
```

Possible problem: the data is not normal. 

# Risk Management Under Non-normal Distributions

## Skewness and Kurtosis

Most common departures from normality:

1. **Skewness:** one tail is longer than the other. 
2. **Kurtosis:** tails are heavier or thinner than the normal.

```{r}
# install.packages("moments")
library(moments)
rvec <- as.vector(logret)
round(skewness(rvec), 2)
```

If the coefficient of skewness is negative, it is left-skewed.

Heavy-tailed distributions are called **leptokurtic** and thin-tailed are called **platykurtic**.

Coefficient of kurtosis:

1. $3$ for normal.
2. $<3$ for thin-tailed.
3. $>3$ for heavy-tailed.

```{r}
round(kurtosis(rvec), 2)
```

## The Jarque-Bera test for normality

```{r}
jarque.test(rvec)
```

Thus, reject normality. 

Other tests: QQPlot and Kolmogorov-Smirnov.

## Student-t Distribution

Has one parameter: $\nu$ degrees of freedom.

Mean: $0$.

Variance: $$\frac{\nu}{\nu - 2}$$ for $\nu > 2$ and $\infty$ for $1 < \nu \le 2$, otherwise undefined.

Skewness: $0$ for $\nu > 3$, otherwise undefined.

Kurtosis: $$3 + \frac{6}{\nu - 4}$$ for $\nu > 4$, $\infty$ for $2 < \nu \le 4$, otherwise undefined.

Has heavier tails than the normal. When $\nu \to \infty$, the distribution converges to $N(0,1)$.

We use the standard student-t distribution with standard deviation $1$.

Assume that $\text{logret} = \mu + \sigma \varepsilon$, in which $\varepsilon$ is a rescaled student-t distribution with dof $\nu$. The mean of $\text{logret}$ is $\mu$ and the standard deviation is $\sigma$.

We'll use the Maximum Likelihood Estimation (MLE) to estimate $(\mu, \sigma, \nu)$. For that, we use `fitdistr` function in the "MASS" package.

```{r}
#install.packages("MASS")
library(MASS)
t.fit <- fitdistr(rvec, "t")
round(t.fit$estimate, 6)
```

### Simulation

To estimate VaR and ES for student-t, we use simulation like in the normal case.

```{r}
alpha <- 0.05
RNGkind(sample.kind="Rounding")
set.seed(123789)
library(metRology)

rvec <- rt.scaled(100000, mean = t.fit$estimate[1], sd = t.fit$estimate[2], df = t.fit$estimate[3])
VaR <- quantile(rvec, alpha)
ES <- mean(rvec[rvec < VaR])

round(VaR, 6)
round(ES, 6)
```

### Multi-day Horizon

We'll compute VaR and ES for a 10-day horizon in 3 ways.

1. Simulate from the estimated student-t distribution.

```{r}
alpha <- 0.05
RNGkind(sample.kind="Rounding")
set.seed(123789)
library(metRology)

rvec <- rep(0, 100000)
for (i in 1:10) {
    rvec <- rvec + rt.scaled(100000, mean = t.fit$estimate[1], sd = t.fit$estimate[2], df = t.fit$estimate[3])
}
# VaR <- quantile(rvec, alpha)
ES <- mean(rvec[rvec < VaR])

round(VaR, 6)
round(ES, 6)
```

2. Simulate from the empirical distribution with i.i.d. draws.

```{r}
alpha <- 0.05
RNGkind(sample.kind="Rounding")
set.seed(123789)
library(metRology)

rvec <- rep(0, 100000)
for (i in 1:10) {
    rvec <- rvec + sample(as.vector(logret), 100000, replace = TRUE)
}
VaR <- quantile(rvec, alpha)
ES <- mean(rvec[rvec < VaR])

round(VaR, 6)
round(ES, 6)
```

3. Simulate from the empirical distribution with block draws.

```{r}
alpha <- 0.05
RNGkind(sample.kind="Rounding")
set.seed(123789)
library(metRology)

rvec <- rep(0, 100000)
rdat <- as.vector(logret)
posn <- seq(from = 1, to = length(rdat) - 9, by = 1)
rpos <- sample(posn, 100000, replace = TRUE)
for (i in 1:10) {
    rvec <- rvec + rdat[rpos]
    rpos <- rpos + 1
}
VaR <- quantile(rvec, alpha)
ES <- mean(rvec[rvec < VaR])

round(VaR, 6)
round(ES, 6)
```

# Risk Management under Volatility Clustering

## Serial Correlation, Volatility Clustering, and GARCH

*Assumption 1:* the future distribution of the log returns is the same as its historical distribution. 

*Assumption 2:* the parameters of the historical distribution are estimated without paying attention to the ordering of the data, i.e., the ordering of the data is not important.

To test if the ordering of the data is important, we'll use some tests. The first is serial correlation.

### Serial correlation

Let $x_t$ be the value of a time series on day $t$.

The **autocorrelation coefficient** at lag $j$ is given by
$$
\rho_j = \text{cor}(x_t, x_{t-j})
$$

And the **autocorrelation coefficient function (ACF)** is the graph of $\rho_j$ for $j = 0, 1, \ldots$. Notice that $\rho_0 = 1$.

```{r}
acf(logret)
```

If most values are outside the dash lines (95% confidence level), then there's evidence of strong correlation.

### Volatility Clustering

High (low) volatility tends to be followed by high (low) volatility.

Here we use $x_t$ as before and define the autocorrelation coefficient at lag $j$ by 
$$
\rho_{|j|} = \text{cor}(|x_t|, |x_{t-j}|)
$$knitr::opts_chunk$set(eval=FALSE)

Then apply `acf()` to the absolute values. 

```{r}
acf(abs(logret))
```

### GARCH - A volatility prediction model

The GARCH(1,1) normal model:

- Mean equation: $r_t = a_0 + \sqrt{h_t} \varepsilon_t$.
- Variance equation: $h_t = \alpha_0 + \beta_1 h_{t-1} + \alpha_1 \varepsilon_{t-1}^2$.
- Distribution equation: $\varepsilon_t \sim N(0,1)$.

Where $r_t$ is the return series with time varying volatility, $a_0$ is its expected return (typically close to $0$), $\sqrt{h_t} \varepsilon_t$ is the unexpected return, with $h_t$ the predictable variance.

If we have constant variance, with $\beta_1 = 0 = \alpha_1$, then the model gives $H_t = \alpha_0$, which is the normal model of log returns: $r_t = \mu + \sigma \varepsilon_t$. 

```{r}
library(rugarch)
garch.N <- ugarchspec(variance.model = list(model = "sGARCH", garchOrder = c(1,1)),
                      mean.model = list(armaOrder = c(0,0), include.mean = TRUE),
                      distribution.model = "norm")
fit.garch.N <- ugarchfit(spec = garch.N, data = logret)
fit.garch.N
```

Save fitted values:

```{r}
save1 <- cbind(logret, fit.garch.N@fit$sigma, fit.garch.N@fit$z)
names(save1) <- c("logret", "s", "z")
```

Diagnostic test: the $z_t$ (fitted value for $\varepsilon_t$) must be normal. Compare mean, sd, skewness, and kurtosis. The Jarque-Bera test can also be used.

Also check the serial correlation.

```{r}
acf(save1$z)
```

```{r}
acf(abs(save1$z))
```

Now, we change the distribution equation from the normal to the rescaled student-t distribution. Thus, the distribution model becomes
$$
\varepsilon_t \sim \frac{t(\nu)}{\sqrt{\frac{\nu}{\nu - 2}}}
$$

```{r}
garch.t <- ugarchspec(variance.model = list(model = "sGARCH", garchOrder = c(1,1)),
                      mean.model = list(armaOrder = c(0,0), include.mean = TRUE),
                      distribution.model = "std")
fit.garch.t <- ugarchfit(spec = garch.t, data = logret)
fit.garch.t
```

```{r}
save1 <- cbind(logret, fit.garch.t@fit$sigma, fit.garch.t@fit$z)
names(save1) <- c("logret", "s", "z")
parm1 <- fit.garch.t@fit$coef
```

## VaR and ES for GARCH Bootstrap

### The ugarchboot Function

```{r}
RNGkind(sample.kind="Rounding")
set.seed(123789)

boot.garch <- ugarchboot(fit.garch.t, 
             method = "Partial",
             sampling = "raw",
             n.ahead = 1,
             n.bootpred = 100000,
             solver = "solnp")

rvec <- boot.garch@fseries
alpha <- 0.05
VaR <- quantile(rvec, alpha)
ES <- mean(rvec[rvec < VaR])
```

### The ugarchrool Function

```{r}
library(bigrquery)
n2016 <- length(logret["1980-01-01/2016-12-31"])
roll.garch <- ugarchroll(spec = garch.t,
              data = logret,
              n.ahead = 1,
              forecast.length = 1,
              n.start = n2016,
              refit.every = 1,
              refit.window = "recursive",
              calculate.VaR = TRUE,
              VaR.alpha = 0.05,
              keep.coef = TRUE)
```
